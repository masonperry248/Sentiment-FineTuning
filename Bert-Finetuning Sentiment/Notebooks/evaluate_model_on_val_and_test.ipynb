{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Predict on validation set\n",
        "model.eval()\n",
        "val_preds, val_true = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "        val_preds.extend(preds)\n",
        "        val_true.extend(labels)\n",
        "\n",
        "# Step 2: Predict on test set\n",
        "test_preds, test_true = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "        test_preds.extend(preds)\n",
        "        test_true.extend(labels)\n",
        "\n",
        "# Step 3: Compute metrics\n",
        "val_acc = accuracy_score(val_true, val_preds)\n",
        "test_acc = accuracy_score(test_true, test_preds)\n",
        "\n",
        "val_report = classification_report(val_true, val_preds, output_dict=True)\n",
        "test_report = classification_report(test_true, test_preds, output_dict=True)\n",
        "\n",
        "# Step 4: Print accuracy and reports\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"\\nValidation Classification Report:\")\n",
        "print(classification_report(val_true, val_preds))\n",
        "\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(test_true, test_preds))\n",
        "\n",
        "# Step 5: F1-score comparison table\n",
        "val_f1 = f1_score(val_true, val_preds, average=None)\n",
        "test_f1 = f1_score(test_true, test_preds, average=None)\n",
        "\n",
        "f1_df = pd.DataFrame({\n",
        "    \"Class\": [f\"Class {i}\" for i in range(len(val_f1))],\n",
        "    \"Validation F1\": val_f1,\n",
        "    \"Test F1\": test_f1\n",
        "})\n",
        "print(\"\\nF1 Score Comparison Table:\")\n",
        "print(f1_df)\n",
        "\n",
        "# Step 6: Accuracy comparison plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Validation\", \"Test\"], [val_acc, test_acc], color=[\"skyblue\", \"salmon\"])\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation vs Test Accuracy\")\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5xF7VFeNjkvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}