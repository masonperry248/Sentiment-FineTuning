# -*- coding: utf-8 -*-
"""evaluate_model_on_val_and_test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRDl9sJRLM2BKk2_2B62dUuo45eIpj0j
"""

import torch
from sklearn.metrics import classification_report, accuracy_score, f1_score
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Predict on validation set
model.eval()
val_preds, val_true = [], []
with torch.no_grad():
    for batch in val_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        labels = batch["labels"].cpu().numpy()
        val_preds.extend(preds)
        val_true.extend(labels)

# Step 2: Predict on test set
test_preds, test_true = [], []
with torch.no_grad():
    for batch in test_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        labels = batch["labels"].cpu().numpy()
        test_preds.extend(preds)
        test_true.extend(labels)

# Step 3: Compute metrics
val_acc = accuracy_score(val_true, val_preds)
test_acc = accuracy_score(test_true, test_preds)

val_report = classification_report(val_true, val_preds, output_dict=True)
test_report = classification_report(test_true, test_preds, output_dict=True)

# Step 4: Print accuracy and reports
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

print("\nValidation Classification Report:")
print(classification_report(val_true, val_preds))

print("\nTest Classification Report:")
print(classification_report(test_true, test_preds))

# Step 5: F1-score comparison table
val_f1 = f1_score(val_true, val_preds, average=None)
test_f1 = f1_score(test_true, test_preds, average=None)

f1_df = pd.DataFrame({
    "Class": [f"Class {i}" for i in range(len(val_f1))],
    "Validation F1": val_f1,
    "Test F1": test_f1
})
print("\nF1 Score Comparison Table:")
print(f1_df)

# Step 6: Accuracy comparison plot
plt.figure(figsize=(6, 4))
plt.bar(["Validation", "Test"], [val_acc, test_acc], color=["skyblue", "salmon"])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Validation vs Test Accuracy")
plt.grid(axis='y')
plt.show()